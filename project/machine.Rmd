---
title: "Machine Learning Project"
output: html_document
---

#Executive Summary
The random forest algorithm did the best job of predicting the manner in which the participants performed the weight lifting exercise. After experimenting with several algorithms: K Nearest Neighbors, Gradient Boosting Machine, Quadratic Discriminant Analysis, and Support Vector Machine, Random Forest had the best accuracy. I also tried different varieties of Random Forest: Weighted Subspace, RFERNS, and Regularized - none performed better than the basic algorithm.

The data set provided included 159 predictors. I eliminated predictors that were unrelated to the exercise (e.g., time), sparsely populated (missing and NA), highly correlated, and identified by recursive feature elimination. In the end, 40 predictors were used in the model. 

The estimated out of sample error is 0.7% based on Random Forest Out of Bag error estimate (0.54%), the lower bound of the 95th percentile confidence band on the test set (99.03%), and the accuracy on the validation data (99.34%).

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(12345)

library(AppliedPredictiveModeling)
library(caret); 
library(gbm);
library(ggplot2); 
library(kernlab);
library(MASS); 
library(pgmm); 
library(randomForest)
library(RANN)
library(mlbench)
library(Hmisc)

# common functions
classAccuracy = function(values,prediction){
  round(sum(prediction == values)/length(values),4)}

foldDataSet = function(dframe, fold, index) {
  x <- fold[index]
  u <- unlist(x, use.names = FALSE)
  return(list(train=dframe[u,], test=dframe[-u,]))
}

# Create data sets and folds
folds <- 5

rawdata <- read.csv("~/Downloads/pml-training.csv")
testdata <- read.csv("~/Downloads/pml-testing.csv")

# Even though I used set.seed(12345) here and from a script, I received
# two different splits of the data. To work around the problem I saved
# the data partition indices and the folds
load("~/coursera/machinelearning/project/inplay.Rda")

#inPlay <-createDataPartition(y=rawdata$classe, p=0.8, list=FALSE)
playground <- rawdata[inPlay,]
valdata <- rawdata[-inPlay,]
#trainfolds <- createFolds(y=playground$classe, k=folds, list=TRUE, returnTrain=TRUE)

# Initialize variables
mymodels <- list(
  "RF"="rf"
  # "WSRF"="wsrf",
  # "RFERNS"="rFerns",
  # "RRF"="RRF"
  # "GBM"="gbm"
  # "KNN"="knn",
  # "QDA"="qda",
  # "SVM"="svmRadial"
)
resultTrain <- NULL
resultTest <- NULL
resultVal <- NULL


features <- list()
kfitlist <- list()
kpredlist <- list()
ktruth <- list()
kcm <- list()
vkcm <- list()

# Feature elimination from Observation and Recursive Feature Elimination
rmFeatures <- c(1:7, 38, 62, 67, 85, 151, 160)

# Clean up data
# get rid of indices & timestamps that may be correlated with the response
# but will not be in the test data set, so start at 8 and remove response
# Remove sparse and near zero variance columns
cleanData = function(dframe) {
  naCount <- vector()
  sparse = NULL
  dframe <- dframe[,-rmFeatures]
  for(j in 1:length(dframe)) {
    # Eliminate sparsely populated (labeled as factors) and non-numeric fields
    N <- nrow(dframe)
    if (!is.numeric(dframe[,j])) { sparse <- c(sparse,j)} else {
      naCount[j] = 0
      for (i in 1:N) {
        if(is.null(dframe[i,j]) || is.na(dframe[i,j])) {
          naCount[j] = naCount[j]+1
          }
        }
      if(naCount[j] > N/2) {sparse <- c(sparse,j)}
      }
    }
  dframe <- dframe[,-sparse]
  #    nzv <- nearZeroVar(dframe)
  #    if(length(nzv)>0) {dframe <- dframe[,-nzv]}
  #    imputeObj <- preProcess(dframe,method="knnImpute")
  #    dframe <- predict(imputeObj, dframe)
  res <- NULL
  res$sparse <- sparse
  #    res$nzv <- nzv
  #    res$impute <- imputeObj
  res$dframe <- dframe
  return(res)      
}

# Mimic what was done to the training set on the test set
cleanTestSet = function(cleanObj) {
  dframe <- cleanObj$dframe; 
  sparse <- cleanObj$sparse; 
  #    nzv <- cleanObj$nzv
  #    imputeObj <- cleanObj$impute
  filt <- cleanObj$filt; 
  pca <- cleanObj$pca
  rfe <- cleanObj$rfe
  
  dframe <- dframe[,-rmFeatures]
  dframe <- dframe[,-sparse]
  #    if(length(nzv)>0) {dframe <- dframe[,-nzv]}
  #    dframe <- predict(imputeObj, dframe)
  if(setcor) {dframe <- dframe[,-filt]}
  if(!is.null(pca)) {dframe <- predict(pca,dframe)}
  if(setrfe && length(rfe)>0) {dframe <- dframe[,rfe]}
  return(dframe)      
}

setpca <- FALSE
setcor <- TRUE
setrfe <- FALSE

for (k in 1:folds) {
  res <- foldDataSet(playground, trainfolds, k)
  training <- res$train; testing <- res$test
  dim(training); dim(testing); 
  
  # Clean training data
  clean <- cleanData(training)
  cleanTrain <- clean$dframe
  
  # Determine Features
  # Filter out highly correlated variables
  if (setcor) {
    corrMatrix <-  cor(cleanTrain)
    highCorr <- findCorrelation(corrMatrix, cutoff = .9)
    filtTrain <- cleanTrain[,-highCorr]
    lowcorrMatrix <- cor(filtTrain)
    summary(corrMatrix[upper.tri(corrMatrix)])
    summary(lowcorrMatrix[upper.tri(lowcorrMatrix)])
    clean$filt <- highCorr # set filter to remove same fields from test data set
  } else { filtTrain <- cleanTrain }
  
  # Principal Component Analysis
  if(setpca) {
    pca <- preProcess(filtTrain,method="pca")
    featTrain <- predict(pca,filtTrain)
    clean$pca <- pca
  } else {featTrain <- filtTrain; clean$pca <- NULL}
  
  # Recursive Feature Elimination
  #subsets <- c(29,31,33,35,37,39,41,43)
  if (setrfe) {
    N <- ncol(featTrain)
    subsets <- NULL
    for (j in 1:(N/3)) {subsets <- c(subsets, as.integer(2*N/3)+j-1)}
    ctrl <- rfeControl(functions = rfFuncs, method = "repeatedcv", repeats = 5,
                       returnResamp = "all", verbose = FALSE)
    rfeObj <- rfe(filtTrain, training$classe, sizes = subsets, 
                  metric="Accuracy", rfeControl = ctrl)
    rfePred <- predictors(rfeObj)
    if (length(rfePred) > 0) {
      featTrain <- featTrain[,rfePred]
      clean$rfe <- rfePred
    }
  }
  
  features[[k]] <- colnames(featTrain)
  # Run Models
  
  # Prep test data set
  clean$dframe <- testing
  cleanTest <- cleanTestSet(clean)
  
  # Predict and Evaluate Results
  set.seed(6789*k)
  classe <- training$classe
  #dframe <- featTrain
  load("~/coursera/machinelearning/project/kfitlist.Rda")
  
  predTrain <- predict(kfitlist[[k]], training)
  accTrain <- classAccuracy(classe, predTrain)
  kpredlist[[k]] <- predict(kfitlist[[k]], testing)
  accTest <- classAccuracy(testing$classe, kpredlist[[k]])
  ktruth[[k]] <- testing$classe
  kcm[[k]] <- confusionMatrix(kpredlist[[k]], testing$classe)
  resultTrain <- rbind(resultTrain,accTrain)
  resultTest <- rbind(resultTest,accTest)
}
colnames(resultTrain) <- c(names(mymodels))
colnames(resultTest) <- c(names(mymodels))

# Run on Validation Test Set
# Get superset of features
finalFeatures <- NULL
for (k in 1:folds) {
   finalFeatures <- c(finalFeatures, features[[k]])
}
finalFeatures <- unique(finalFeatures)

#clean$dframe <- valdata
#cleanValdata <- cleanTestData(clean) #not needed if not transforming variables
vpredlist <- list()
for (k in 1:folds) {
  fit <- kfitlist[[k]]
  vpredlist[[k]] <- predict(fit, valdata)
  accVal <- classAccuracy(valdata$classe, vpredlist[[k]])
  resultVal <- rbind(resultVal,accVal)
  vkcm[[k]] <- confusionMatrix(vpredlist[[k]], valdata$classe)
}

# Project Output
resultOOB <- NULL
resultMtry <- NULL
resultPred <- NULL
resultKappa <- NULL
resultAccLow <- NULL
for (k in 1:folds) {
  fit <- kfitlist[[k]]
  resultAccLow <- c(resultAccLow, round(vkcm[[k]]$overall[3],4))
  resultOOB <- c(resultOOB,round(fit$finalModel$err.rate[500,"OOB"],6))
  resultKappa <- c(resultKappa, round(vkcm[[k]]$overall[2],4))
  resultMtry <- c(resultMtry, fit$finalModel$mtry)
  resultPred <- c(resultPred, nrow(fit$finalModel$importance))

}
resultTab <- cbind(resultTest, resultAccLow, resultOOB, resultVal, 
                   resultKappa, resultMtry, resultPred)
rownames(resultTab) <- c("1", "2", "3", "4", "5")
colnames(resultTab) <- c("Test Accuracy", "Low 95th Acc", "OOB Error", 
                         "Val Accuracy", "Kappa", "MTRY", "Predictors")


# Important Variables
bot5Features <- NULL
top5Features <- NULL
for (k in 1:folds) {
  imp <- varImp(fit, type=1)$importance # gives overall importance
  impOrd <- imp[order(-imp$Overall),, drop=FALSE]
  imp5 <- rownames(impOrd[1:5,, drop=FALSE])
  impB5 <- rownames(impOrd[(nrow(imp)-4):nrow(imp),, drop=FALSE])
  bot5Features <- c(bot5Features, impB5)
  top5Features <- c(top5Features, imp5)
}
bot5Features <- unique(bot5Features)
top5Features <- unique(top5Features)

# Final Model
fit <- kfitlist[[5]]
imp <- varImp(fit, type=1)$importance # gives overall importance
impOrd <- imp[order(-imp$Overall),, drop=FALSE]
imp5 <- rownames(impOrd[1:5,, drop=FALSE])
impB5 <- rownames(impOrd[(nrow(imp)-5):nrow(imp),, drop=FALSE])

#load("~/coursera/machinelearning/project/rfe.Rda")
#rfePred <- predictors(rfeObj)
#delta <- setdiff(colnames(training),rfePred)
```

# Predictor Components
## Question
From the project assignment, the "goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants ... to predict the manner in which they did the exercise." Therefore, accuracy is the key measure of success for the prediction model.

## Input Data
The data included 159 predictors and an outcome, and 19622 samples. To develop and validate a model, I randomly separated the samples into two using the "createDataPartition" utility, 80% for model development, and 20% for validation. Five folds were created from the model development samples using "createFolds", each fold with training and test data sets. Altogether, the split was roughly 64% training, 16% test, 20% validation. I performed the vast majority of the feature and model selection on the first training set, and then fine tuned the model using all of the training sets.

## Feature Selection
### Inspection
On inspection, the first 7 predictors had nothing to do with the exercise, and could be misleading (e.g., index and outcome correlation), therefore I eliminated them. Here's the list of predictors eliminated based on observed relevance:
```{r, eval=TRUE,echo=FALSE}
colnames(training)[1:7]
```

Looking further into the data, I noticed 100 predictors with 11560 NA or Missing values from the training data set - more than 90% of the samples. Imputing values for the missing data from a small population didn't make sense, so I eliminated the sparsely populated predictors (too many to list).

```{r, eval=FALSE,echo=FALSE}
dframe <- training[,-rmFeatures]
colnames(dframe)[clean$sparse]
```

I looked at a few scatter plots and boxplots. There were too many scatter plots to make sense of, and reviewing the box plots of what remained after cleaning the data didn't reveal obvious candidates for removal. Below are examples of boxplots for the 2 most important predictors:
```{r, echo=FALSE,fig.width=7,fig.height=4}
#par(mfrow = c(1,1))
#classe <- training$classe
#pdata <- subset(dframe, classe=="A" | classe=="D")
#featurePlot(x=pdata[,imp5], y=pdata$classe, plot="pairs", 
#            auto.key=list(space="top", columns=5, title="Class", cex.title=1))
par(mfrow = c(1,2))
for(var in imp5[1:2])  {
  plot(training[, var] ~ training$classe, col=c(2:6), xlab="Class",ylab=var)
}
```

With the data cleaned, I narrowed the features to be included based on correlation and recursive feature elimination.

### Zero Variance and High Correlation
I tried Caret's Near Zero Variance (NZV) tool to eliminate predictors, but it didn't identify any predictors with near zero variance. Thus, NZV wasn't used for feature selection.

Eliminating highly correlated predictors improved accuracy. Caret's findCorrelation tool was used to identify features with correlations greater than .9. Using a correlation threshold of .8 or 1 decreased accuracy. I didn't fine tune based on the correlation threshold. Here's a list of predictors eliminated based on high correlation with other predictors:

```{r, eval=TRUE,echo=FALSE}
colnames(cleanTrain)[highCorr]
```

### Principal Component Analysis
Principal Component Analysis yielded a drop in Accuracy from 99% to 95%, therefore it wasn't used to select features.

### Recursive Feature Elimination
Caret's Recursive Feature Elimination (RFE) tool helped eliminate features that did not affect accuracy. Here's the list of predictors RFE eliminated along with the r-code:
```{r, eval=TRUE,echo=FALSE}
rfe <- c(38, 62, 67, 85, 151)
colnames(training)[rfe]
```
```{r, eval=FALSE,echo=TRUE}
N <- ncol(filtTrain)
subsets <- NULL
for (j in 1:(N/3)) {subsets <- c(subsets, as.integer(2*N/3)+j-1)}
ctrl <- rfeControl(functions = rfFuncs, method = "repeatedcv", repeats = 5,
                   returnResamp = "all", verbose = FALSE)
rfeObj <- rfe(filtTrain, training$classe, sizes = subsets, 
              metric="Accuracy", rfeControl = ctrl)
rfePred <- predictors(rfeObj)
```
### Final Feature Set
The final list of features in the model, in order of importance, are:
```{r, eval=TRUE,echo=FALSE}
impOrd
```

## Algorithm
I tried several algorithms on the first training fold. Random Forest had the best accuracy (99%). The others I tried were (with accuracy in parentheses): Gradient Boosting Machine (98%), K Nearest Neighbors (95%), Quadratic Discriminant Analysis (92%), and Support Virtual Machine Radial (81%). Given the success with Random Forest, I tried the following variants: Weighted Subspace (98%), RFERNS (83%), and Regularized (never finished) - none performed as well as the basic Random Forest algorithm. 

Combinations of RF, GBM, and KNN did no better than RF. The same was true for the combination of RF and WSRF. Furthermore, combining the fits from the 5-folds didn't improve accuracy either. Thus, I didn't use a combined model.

Once I settled on the RF algorithm, I tuned the mtry parameter. 

## Model Parameters
The mtry parameter, number of predictors sampled for splitting at each node, is configurable for "rf". From experimentation, the range of values caret (i.e. train) selected was 7 to 12, so I ran the models with a range of 6 to 14 to make sure I found the mtry that yielded the greatest accuracy. Here's the R code used to fit the model.
```{r, eval=FALSE,echo=TRUE}
fit <- train(classe ~ ., method="rf", data=dframe,
                  tuneGrid=data.frame(mtry=c(6,7,8,9,10,11,12,13,14)),
                  importance = TRUE)
```
 
I also tried increasing the number of trees from the default of 500, but it decreased the accuracy and increased the runtime.
 
## Evaluation and Cross-Validation
Below is a table of results from the 5 folds showing the Accuracy, Low 95th % accuracy bound, Out of Bag Error Rate, Accuracy against the Validation Data Set, Kappa, mtry, and predictor variables (changed depending on covariance of predictors).

```{r, eval=TRUE,echo=FALSE}
resultTab
```

Selecting the final model based on just accuracy could yield sub-optimal performance, in particular the confidence of the estimate has to be taken into account. Ultimately, I chose the fit from the 5th fold because the OOB Error estimate was good, the lower bound of the 95th percentile of accuracy was the highest, the Kappa score was the highest, and the performance against the validation data set was the best.

Below is the confusion matrix for the final fit against the validation data:
```{r, eval=TRUE,echo=FALSE}
vkcm[[5]]$table
```

## Error Estimate
The random forest OOB error estimate is 0.54%, the lower bound 95th percentile accuracy on the test set was 99.03%, and the accuracy on the validation data set was 99.34%. From the 5-fold cross-validation, I found the $Low 95th Percentile Accuracy + OOB/2$ on the test set to be a good estimate of the validation error. Thus, my error estimate is 0.7%

# Conclusion
The Caret Random Forest model fit of the 5th fold with 40 predictors (listed earlier) and an mtry of 9 is expected to have an error rate of 0.7%

# References
* [ISLR](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Fourth%20Printing.pdf)
* [Caret](http://topepo.github.io/caret/index.html)
* [Random Forest](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&ved=0CB4QFjAA&url=http%3A%2F%2Fcran.r-project.org%2Fweb%2Fpackages%2FrandomForest%2FrandomForest.pdf&ei=lKA1VZzkBYiAsQSMpYCAAg&usg=AFQjCNFPbPBBJkcCnDl5YWtC0SlLpRiXRQ&sig2=H6Xy-IQqmo88zFt_8Lza7w)